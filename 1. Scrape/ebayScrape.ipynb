{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2027422b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Search</th>\n",
       "      <th>Product Title</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Price</th>\n",
       "      <th>Seller</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review - Title</th>\n",
       "      <th>Review - Star</th>\n",
       "      <th>Review - Author</th>\n",
       "      <th>Review - Date</th>\n",
       "      <th>Review - Verified Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iphone</td>\n",
       "      <td>Apple iPhone 13 Pro Max - 128GB - Alpine Green...</td>\n",
       "      <td>musicmagpie</td>\n",
       "      <td>¬£634.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Probably as I had hoped for as the phone is in...</td>\n",
       "      <td>Great but heavy.</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>stickyprobs</td>\n",
       "      <td>28 Jun, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iphone</td>\n",
       "      <td>Apple iPhone 13 Pro Max - 128GB - Alpine Green...</td>\n",
       "      <td>new</td>\n",
       "      <td>¬£634.99</td>\n",
       "      <td>ssa2234ssa</td>\n",
       "      <td>Fantastic,new and sealed as described. It come...</td>\n",
       "      <td>Fantastic</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>helwa1078-voxze...</td>\n",
       "      <td>29 Sep, 2022</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iphone</td>\n",
       "      <td>Apple iPhone 13 Pro Max - 128GB - Alpine Green...</td>\n",
       "      <td>pre-owned</td>\n",
       "      <td>¬£634.99</td>\n",
       "      <td>phones2gadgets</td>\n",
       "      <td>It‚Äôs  a good phone.. Had a new 13 model but fo...</td>\n",
       "      <td>Will do for me for a few years.. hopefully..</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>paulprior249</td>\n",
       "      <td>20 Jun, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iphone</td>\n",
       "      <td>Apple iPhone 13 Pro Max - 128GB - Alpine Green...</td>\n",
       "      <td>phonesdirect_com</td>\n",
       "      <td>¬£634.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perfect quality as if its new and it has 100% ...</td>\n",
       "      <td>Goood</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>88s275</td>\n",
       "      <td>16 Aug, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iphone</td>\n",
       "      <td>Apple iPhone 13 Pro Max - 128GB - Alpine Green...</td>\n",
       "      <td>new</td>\n",
       "      <td>¬£634.99</td>\n",
       "      <td>sell-phones</td>\n",
       "      <td>I love it fantastic phone</td>\n",
       "      <td>Perfect üëç</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>sarolga</td>\n",
       "      <td>04 Jun, 2022</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62642</th>\n",
       "      <td>Apple Watch</td>\n",
       "      <td>Bidding ended on Thu, 23 Nov at 11:00 AM.</td>\n",
       "      <td>pre-owned</td>\n",
       "      <td>¬£195.00</td>\n",
       "      <td>davdivjak</td>\n",
       "      <td>Amazing very pleased with it had no problems w...</td>\n",
       "      <td>Good buy</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>bigtez1</td>\n",
       "      <td>11 Aug, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62643</th>\n",
       "      <td>Apple Watch</td>\n",
       "      <td>Bidding ended on Thu, 23 Nov at 11:00 AM.</td>\n",
       "      <td>new</td>\n",
       "      <td>¬£195.00</td>\n",
       "      <td>marstan57</td>\n",
       "      <td>You know what you‚Äôre getting with an apple pro...</td>\n",
       "      <td>Fantastic watch</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>kevicarte-7</td>\n",
       "      <td>08 Jun, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62644</th>\n",
       "      <td>Apple Watch</td>\n",
       "      <td>Bidding ended on Thu, 23 Nov at 11:00 AM.</td>\n",
       "      <td>new</td>\n",
       "      <td>¬£195.00</td>\n",
       "      <td>gogipat</td>\n",
       "      <td>Amazing</td>\n",
       "      <td>Gry</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>stan3530</td>\n",
       "      <td>12 Sep, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62645</th>\n",
       "      <td>Apple Watch</td>\n",
       "      <td>Bidding ended on Thu, 23 Nov at 11:00 AM.</td>\n",
       "      <td>new</td>\n",
       "      <td>¬£195.00</td>\n",
       "      <td>shelly1719</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very good</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>semifav0</td>\n",
       "      <td>12 May, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62646</th>\n",
       "      <td>Apple Watch</td>\n",
       "      <td>Bidding ended on Thu, 23 Nov at 11:00 AM.</td>\n",
       "      <td>pre-owned</td>\n",
       "      <td>¬£195.00</td>\n",
       "      <td>the_phone_centr...</td>\n",
       "      <td>Bargain üëå</td>\n",
       "      <td>Bargain just like brand new</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>jucylucy79</td>\n",
       "      <td>26 Feb, 2023</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62647 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Search                                      Product Title  \\\n",
       "0           Iphone  Apple iPhone 13 Pro Max - 128GB - Alpine Green...   \n",
       "1           Iphone  Apple iPhone 13 Pro Max - 128GB - Alpine Green...   \n",
       "2           Iphone  Apple iPhone 13 Pro Max - 128GB - Alpine Green...   \n",
       "3           Iphone  Apple iPhone 13 Pro Max - 128GB - Alpine Green...   \n",
       "4           Iphone  Apple iPhone 13 Pro Max - 128GB - Alpine Green...   \n",
       "...            ...                                                ...   \n",
       "62642  Apple Watch          Bidding ended on Thu, 23 Nov at 11:00 AM.   \n",
       "62643  Apple Watch          Bidding ended on Thu, 23 Nov at 11:00 AM.   \n",
       "62644  Apple Watch          Bidding ended on Thu, 23 Nov at 11:00 AM.   \n",
       "62645  Apple Watch          Bidding ended on Thu, 23 Nov at 11:00 AM.   \n",
       "62646  Apple Watch          Bidding ended on Thu, 23 Nov at 11:00 AM.   \n",
       "\n",
       "              Condition    Price              Seller  \\\n",
       "0           musicmagpie  ¬£634.99                 NaN   \n",
       "1                   new  ¬£634.99          ssa2234ssa   \n",
       "2             pre-owned  ¬£634.99      phones2gadgets   \n",
       "3      phonesdirect_com  ¬£634.99                 NaN   \n",
       "4                   new  ¬£634.99         sell-phones   \n",
       "...                 ...      ...                 ...   \n",
       "62642         pre-owned  ¬£195.00           davdivjak   \n",
       "62643               new  ¬£195.00           marstan57   \n",
       "62644               new  ¬£195.00             gogipat   \n",
       "62645               new  ¬£195.00          shelly1719   \n",
       "62646         pre-owned  ¬£195.00  the_phone_centr...   \n",
       "\n",
       "                                                  Review  \\\n",
       "0      Probably as I had hoped for as the phone is in...   \n",
       "1      Fantastic,new and sealed as described. It come...   \n",
       "2      It‚Äôs  a good phone.. Had a new 13 model but fo...   \n",
       "3      Perfect quality as if its new and it has 100% ...   \n",
       "4                              I love it fantastic phone   \n",
       "...                                                  ...   \n",
       "62642  Amazing very pleased with it had no problems w...   \n",
       "62643  You know what you‚Äôre getting with an apple pro...   \n",
       "62644                                            Amazing   \n",
       "62645                                                Yes   \n",
       "62646                                          Bargain üëå   \n",
       "\n",
       "                                     Review - Title Review - Star  \\\n",
       "0                                  Great but heavy.       4 stars   \n",
       "1                                         Fantastic       5 stars   \n",
       "2      Will do for me for a few years.. hopefully..       5 stars   \n",
       "3                                             Goood       5 stars   \n",
       "4                                         Perfect üëç       5 stars   \n",
       "...                                             ...           ...   \n",
       "62642                                      Good buy       5 stars   \n",
       "62643                               Fantastic watch       5 stars   \n",
       "62644                                           Gry       5 stars   \n",
       "62645                                     Very good       5 stars   \n",
       "62646                   Bargain just like brand new       5 stars   \n",
       "\n",
       "          Review - Author Review - Date Review - Verified Purchase  \n",
       "0             stickyprobs  28 Jun, 2023                        Yes  \n",
       "1      helwa1078-voxze...  29 Sep, 2022                        Yes  \n",
       "2            paulprior249  20 Jun, 2023                        Yes  \n",
       "3                  88s275  16 Aug, 2023                        Yes  \n",
       "4                 sarolga  04 Jun, 2022                        Yes  \n",
       "...                   ...           ...                        ...  \n",
       "62642             bigtez1  11 Aug, 2023                        Yes  \n",
       "62643         kevicarte-7  08 Jun, 2023                        Yes  \n",
       "62644            stan3530  12 Sep, 2023                        Yes  \n",
       "62645            semifav0  12 May, 2023                        Yes  \n",
       "62646          jucylucy79  26 Feb, 2023                        Yes  \n",
       "\n",
       "[62647 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = \"Data/ebay_comments.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c7b39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62647"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = df.to_dict(orient='records')\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c80f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "iphone_url = \"https://www.ebay.co.uk/sch/i.html?_from=R40&_trksid=p4432023.m570.l2632&_nkw=apple+watch&_sacat=178893\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "urls = list()\n",
    "\n",
    "for i in range(167):\n",
    "    print(i)\n",
    "    try:\n",
    "        search_url = iphone_url + str(i+1)\n",
    "\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        temp = [a[\"href\"] for a in soup.find_all(\"a\", class_=\"s-item__link\")]\n",
    "\n",
    "        urls = urls + temp\n",
    "    except:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Filter out non-product links\n",
    "urls = [link for link in urls if \"ebay.co.uk/itm/\" in link]\n",
    "urls = list(set(urls))\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31645bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "#results = []\n",
    "\n",
    "for url in range(len(urls)):\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(urls[url], headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the elements\n",
    "    try:\n",
    "        product_title_element = soup.find('span', {'class': 'ux-textspans ux-textspans--BOLD'})\n",
    "    except:\n",
    "        continue\n",
    "    price_element = soup.find('div', {'data-testid': 'x-price-primary'})\n",
    "    reviews_element = soup.find('div', {'class': 'x-review-details__allreviews'})\n",
    "    \n",
    "    # Get the text from the elements\n",
    "    try:\n",
    "        product_title = product_title_element.get_text(strip=True)\n",
    "    except:\n",
    "        continue\n",
    "    price = price_element.find(class_=\"ux-textspans\").text.strip()\n",
    "    try:\n",
    "        reviews_link = reviews_element.find('a', {'data-testid': 'ux-action'})['href']\n",
    "    except:\n",
    "        continue\n",
    "    reviews_count = int(reviews_element.find('a', {'data-testid': 'ux-action'}).text.strip().split()[2])\n",
    "    if reviews_count % 10 == 0:\n",
    "        pages_count = int(reviews_count/10)\n",
    "    else:\n",
    "        pages_count = int(reviews_count/10)+1\n",
    "    pages_list = list()\n",
    "    \n",
    "    for i in range(pages_count):\n",
    "        page = reviews_link + \"&pgn=\" + str(i+1)\n",
    "        response = requests.get(page, headers=headers)\n",
    "        \n",
    "        html_content = response.content\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all the review elements\n",
    "        review_elements = soup.find_all(class_=\"ebay-review-section\")\n",
    "        \n",
    "\n",
    "        # Loop through the review elements and extract the text\n",
    "        for review_element in review_elements:\n",
    "            try:\n",
    "                review_text = review_element.find(class_=\"review-item-content\").get_text(strip=True, separator='\\n')\n",
    "                \n",
    "            except:\n",
    "                review_text = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_title = review_element.find(class_=\"review-item-title\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_title = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_star = review_element.find(class_=\"star-rating\")['aria-label']\n",
    "            except:\n",
    "                review_star = \"\"\n",
    "            try:\n",
    "                review_author = review_element.find(class_=\"review-item-author\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_author = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_date = review_element.find(class_=\"review-item-date\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_date = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_verifiedPurchase = review_element.find_all(class_=\"rvw-val\")[0].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_verifiedPurchase = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                condition = review_element.find_all(class_=\"rvw-val\")[1].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                condition = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                seller = review_element.find_all(class_=\"rvw-val\")[2].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                seller = \"\"\n",
    "                pass\n",
    "            results.append({'Search': 'Apple Watch', 'Product Title': product_title, 'Condition': condition, 'Price': price, 'Seller': seller, \n",
    "                            'Review': review_text, 'Review - Title': review_title, 'Review - Star': review_star,\n",
    "                            'Review - Author': review_author, 'Review - Date': review_date,  \n",
    "                            'Review - Verified Purchase': review_verifiedPurchase})\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv('ebay_comments.csv', index=False)\n",
    "        time.sleep(2)\n",
    "        \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(results)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('ebay_comments.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Search.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c218ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "iphone_url = \"https://www.ebay.co.uk/sch/58058/i.html?_from=R40&_nkw=macbook&LH_TitleDesc=0\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "urls = list()\n",
    "\n",
    "for i in range(167):\n",
    "    print(i)\n",
    "    try:\n",
    "        search_url = iphone_url + str(i+1)\n",
    "\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        temp = [a[\"href\"] for a in soup.find_all(\"a\", class_=\"s-item__link\")]\n",
    "\n",
    "        urls = urls + temp\n",
    "    except:\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Filter out non-product links\n",
    "urls = [link for link in urls if \"ebay.co.uk/itm/\" in link]\n",
    "urls = list(set(urls))\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "#results = []\n",
    "\n",
    "for url in range(len(urls)):\n",
    "    print(url)\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(urls[url], headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the elements\n",
    "    try:\n",
    "        product_title_element = soup.find('span', {'class': 'ux-textspans ux-textspans--BOLD'})\n",
    "    except:\n",
    "        continue\n",
    "    price_element = soup.find('div', {'data-testid': 'x-price-primary'})\n",
    "    reviews_element = soup.find('div', {'class': 'x-review-details__allreviews'})\n",
    "    \n",
    "    # Get the text from the elements\n",
    "    try:\n",
    "        product_title = product_title_element.get_text(strip=True)\n",
    "    except:\n",
    "        continue\n",
    "    price = price_element.find(class_=\"ux-textspans\").text.strip()\n",
    "    try:\n",
    "        reviews_link = reviews_element.find('a', {'data-testid': 'ux-action'})['href']\n",
    "    except:\n",
    "        continue\n",
    "    reviews_count = int(reviews_element.find('a', {'data-testid': 'ux-action'}).text.strip().split()[2])\n",
    "    if reviews_count % 10 == 0:\n",
    "        pages_count = int(reviews_count/10)\n",
    "    else:\n",
    "        pages_count = int(reviews_count/10)+1\n",
    "    pages_list = list()\n",
    "    \n",
    "    for i in range(pages_count):\n",
    "        page = reviews_link + \"&pgn=\" + str(i+1)\n",
    "        response = requests.get(page, headers=headers)\n",
    "        \n",
    "        html_content = response.content\n",
    "\n",
    "        # Create a BeautifulSoup object\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all the review elements\n",
    "        review_elements = soup.find_all(class_=\"ebay-review-section\")\n",
    "        \n",
    "\n",
    "        # Loop through the review elements and extract the text\n",
    "        for review_element in review_elements:\n",
    "            try:\n",
    "                review_text = review_element.find(class_=\"review-item-content\").get_text(strip=True, separator='\\n')\n",
    "                \n",
    "            except:\n",
    "                review_text = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_title = review_element.find(class_=\"review-item-title\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_title = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_star = review_element.find(class_=\"star-rating\")['aria-label']\n",
    "            except:\n",
    "                review_star = \"\"\n",
    "            try:\n",
    "                review_author = review_element.find(class_=\"review-item-author\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_author = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_date = review_element.find(class_=\"review-item-date\").get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_date = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                review_verifiedPurchase = review_element.find_all(class_=\"rvw-val\")[0].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                review_verifiedPurchase = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                condition = review_element.find_all(class_=\"rvw-val\")[1].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                condition = \"\"\n",
    "                pass\n",
    "            try:\n",
    "                seller = review_element.find_all(class_=\"rvw-val\")[2].get_text(strip=True, separator='\\n')\n",
    "            except:\n",
    "                seller = \"\"\n",
    "                pass\n",
    "            results.append({'Search': 'Macbook', 'Product Title': product_title, 'Condition': condition, 'Price': price, 'Seller': seller, \n",
    "                            'Review': review_text, 'Review - Title': review_title, 'Review - Star': review_star,\n",
    "                            'Review - Author': review_author, 'Review - Date': review_date,  \n",
    "                            'Review - Verified Purchase': review_verifiedPurchase})\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv('ebay_comments.csv', index=False)\n",
    "        time.sleep(2)\n",
    "        \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(results)\n",
    "df = df.drop_duplicates()\n",
    "df.to_csv('ebay_comments.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdc351",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Search.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0045e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce86c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "136117f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iphone         51903\n",
       "AirPods         5231\n",
       "iPad            2354\n",
       "Apple Watch     1828\n",
       "Macbook         1331\n",
       "Name: Search, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Search.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mac\n",
    "iPhone\n",
    "iPad\n",
    "AirPods\n",
    "Watch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
